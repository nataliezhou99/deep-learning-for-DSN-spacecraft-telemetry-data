"""
JWST Prediction Data Utilities
----------------------------------------
Purpose
    Utilities to build sliding‑window sequence datasets and DataLoaders for
    predicting target telemetry features and classifying anomaly presence.

Key behaviors
    • Loads per‑track features from Parquet and optional label masks from NPY.
    • Produces per‑example windows of length `window_size` with the model target
      taken at the last timestep of each window.
    • Binary label for a window is 1 if any timestep within the window is
      labeled anomalous; otherwise 0.
    • Constructs train/val/test DataLoaders from a manifest generated by the
      preprocessing pipeline.

Inputs
    • manifest_path: JSON with entries per split: [{"track_features", "track_labels"}]
    • data_dir: directory that contains the per‑track parquet and label files
    • input_cols: list of column names used as model inputs
    • target_cols: list of column names the model predicts at the last step

Notes
    • No functional changes are introduced; only comments/docstrings/log context
      for production clarity. Names and logic remain intact.
"""

import torch
import numpy as np
import pandas as pd
import logging
import json
from pathlib import Path
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import random

# Unified logging format for downstream trainers and notebooks
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') 


class SingleTrackPredictionDataset(Dataset): 
    """Sliding‑window dataset over a single telemetry track.

    Each item is a tuple: ((input_window[T×F_in], target_last_step[F_out]), label)
    where label∈{0,1} indicates whether any timestep within the window overlaps
    an anomaly.

    Parameters
    ----------
    track_path : str or Path
        Path to a Parquet file containing preprocessed numeric features.
    labels_path : str or Path
        Default path to the .npy labels for this track. Ignored if `label_dir`
        is given (see below).
    input_cols : list[str]
        Column names to serve as model inputs.
    target_cols : list[str]
        Column names for the prediction target at the *last* timestep.
    window_size : int
        Sliding window length (number of timesteps per example).
    label_dir : str or Path, optional
        If provided, the labels path will be formed as
        `<label_dir>/<track_stem><label_suffix>`.
    label_suffix : str
        Suffix to append to track stem when using `label_dir` (default: '_labels.npy').
    """
    def __init__(self, track_path, labels_path, input_cols, target_cols, window_size, 
                 label_dir=None, label_suffix='_labels.npy'):
        self.window_size = window_size
        self.track_path = Path(track_path)
        
        # If label_dir is supplied, derive labels path from track file stem; else use explicit labels_path.
        if label_dir:
            track_id = self.track_path.stem
            self.labels_path = Path(label_dir) / f"{track_id}{label_suffix}"
        else:
            self.labels_path = Path(labels_path)

        # Load features for a single track (preprocessed and aligned columns)
        df = pd.read_parquet(self.track_path)
        
        # Cache input and target arrays as float32 tensors for GPU efficiency
        self.input_features = df[input_cols].to_numpy(dtype=np.float32)
        self.target_features = df[target_cols].to_numpy(dtype=np.float32)

        # Load labels if present; default to zeros (no anomalies) if missing.
        if self.labels_path.exists():
            self.labels = np.load(self.labels_path).astype(np.float32)
        else:
            self.labels = np.zeros((len(self.input_features), 1), dtype=np.float32)

    def __len__(self): 
        """Number of sliding windows available for this track."""
        return max(0, len(self.input_features) - self.window_size + 1) 

    def __getitem__(self, index): 
        """Return ((input_window, target_at_last_step), label) for the given index.

        The window spans [index, index+window_size). The classification label is
        1 if any label within the window is positive.
        """
        window_end = index + self.window_size

        # Feature window across timesteps for model input
        input_window = self.input_features[index:window_end]

        # Regression/forecasting target taken at the last timestep of the window
        target_at_last_step = self.target_features[window_end - 1] 
        
        # Window‑level anomaly label: any‑positive → 1, else 0
        label_window = self.labels[index:window_end]
        label = 1.0 if np.any(label_window) else 0.0
        
        return (torch.tensor(input_window, dtype=torch.float32), 
                torch.tensor(target_at_last_step, dtype=torch.float32)), \
               torch.tensor(label, dtype=torch.float32)


def create_prediction_dataloaders( 
    manifest_path: Path, data_dir: Path, input_cols: list, target_cols: list, 
    batch_size: int, window_size: int, num_workers: int = 0, debug_mode: bool = False, seed: int = 99,
    **kwargs 
):
    """Create train/val/test DataLoaders from a manifest and on‑disk track files.

    Parameters
    ----------
    manifest_path : Path
        JSON manifest with keys 'train' | 'val' | 'test'. Each value is a list of
        objects containing 'track_features' and 'track_labels' relative paths.
    data_dir : Path
        Base directory containing the track parquet files and label npy files.
    input_cols, target_cols : list[str]
        Model input and target columns, respectively.
    batch_size : int
        Batch size for DataLoaders.
    window_size : int
        Sliding window length passed through to the dataset.
    num_workers : int
        PyTorch DataLoader workers. Tune based on I/O and CPU characteristics.
    debug_mode : bool
        If True, shrink dataset sizes for quick iteration.
    seed : int
        Random seed for deterministic debug sampling.
    **kwargs : dict
        Additional args forwarded to SingleTrackPredictionDataset (e.g., label_dir,label_suffix).

    Returns
    -------
    dict
        Mapping split → DataLoader (or None if split is empty).
    """
    logging.info(f"Creating Prediction DataLoaders from manifest: {manifest_path}")

    # Load manifest generated by the preprocessing pipeline
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    random.seed(seed)
    dataloaders = {}
    
    for split in ['train', 'val', 'test']: 
        tracks = manifest[split]

        # Optional downsampling for rapid local debugging
        if debug_mode:
            logging.warning(f"DEBUG MODE: Using a small subset of {split} tracks.")
            random.shuffle(tracks)
            tracks = tracks[:20] if split != 'test' else tracks[:10]

        # Instantiate a per‑track dataset for each item in the split
        datasets = [
            SingleTrackPredictionDataset(
                data_dir / t['track_features'],
                data_dir / t['track_labels'],
                input_cols,
                target_cols,
                window_size,
                **kwargs,
            )
            for t in tracks
        ]

        if not datasets:
            logging.warning(f"No data for split: {split}. Skipping DataLoader creation.")
            dataloaders[split] = None
            continue
            
        # Concat across tracks so each window becomes an independent sample
        full_dataset = ConcatDataset(datasets)
        
        is_train = split == 'train'
        dataloaders[split] = DataLoader(
            full_dataset, batch_size=batch_size, shuffle=is_train,
            num_workers=num_workers, pin_memory=True, drop_last=is_train
        )
        logging.info(f"Created {split} loader with {len(full_dataset)} samples.")

    logging.info("All Prediction DataLoaders created successfully.")
    return dataloaders
